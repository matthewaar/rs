# -*- coding: utf-8 -*-
"""Copy of FASTTEXT_BISMILLAH

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CDDti_K7tscUmPGFDCL2b2ydfOCqrPjo
"""

import pandas as pd
import re

datasets = {
    "ayam": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-ayam.csv",
    "ikan": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-ikan.csv",
    "kambing": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-kambing.csv",
    "sapi": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-sapi.csv",
    "tahu": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-tahu.csv",
    "telur": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-telur.csv",
    "tempe": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-tempe.csv",
    "udang": r"https://raw.githubusercontent.com/Lindaaa0/Dataset/main/dataset-udang.csv",
}

dataframes = []

for kategori, url in datasets.items():
    df = pd.read_csv(url)

    df['kategori'] = kategori

    dataframes.append(df)

merged_data = pd.concat(dataframes, ignore_index=True)

"""## **EDA**"""

merged_data = merged_data.dropna()

merged_data['word_count'] = merged_data['Ingredients'].apply(lambda x: len(x.split()))

"""## **Data Cleansing**"""

merged_data['Ingredients'] = merged_data['Ingredients'].str.lower()
merged_data['Ingredients'] = merged_data['Ingredients'].str.strip()

"""### In-depth Cleansing"""

merged_data['ingredients_list'] = merged_data['Ingredients'].apply(lambda x: x.split('--')[:-1])

#convert the list of items to set to avoid any repeated ingredients
merged_data['ingredients_list']=merged_data['ingredients_list'].apply(set)

#Number of elements in ingredients list
merged_data['ingredient_item']=merged_data['ingredients_list'].str.len()

pd.set_option('display.max_colwidth', None)
merged_data.loc[merged_data['ingredient_item']==merged_data['ingredient_item'].max()]

merged_data['ingredients'] = merged_data['ingredients_list'].apply(lambda x: ' ;'.join(map(str, x)))

## specify which columns
merged_data = merged_data[['Title', 'ingredients_list', 'Steps', 'ingredient_item', 'ingredients', 'kategori', 'Loves']]

"""Cek satu satu untuk kasus item 1-5 ada anomali kaya diatas atau ngga, tapi pas dicek untuk ingredients 2-5 gaada masalah. yang ada masalah di bagian ketika itemnys 1 aja

Karena datanya cmn 2 yang anomali kita apus aja
"""

merged_data.drop(labels=merged_data.loc[merged_data['ingredient_item']==merged_data['ingredient_item'].min()].index, axis=0, inplace=True )

"""cek lagi utk steps atau intruksi"""

merged_data['recipe_words']=merged_data['Steps'].apply(lambda x: len(x.split()))

"""aman, krna gaada steps yang kosong. dan paling kecil cmn 5 yang dimana buat instruksi masi bs masuk diakal klo cmn ada 5 steps

ada yang duplikat ternyata gais, mari kita cek

karena bener ada yang duplikat jadinya kita apus duplikatnya ya
"""

merged_data.drop(columns='ingredients_list', inplace=True)
merged_data.drop_duplicates(keep='first', inplace=True)

"""jadi data yang kita pake ada segitu ya gengs

### Improved Handling Manual
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
import string

nltk.download("punkt", quiet=True)
nltk.download("wordnet", quiet=True)
nltk.download("stopwords", quiet=True)
nltk.download('omw-1.4', quiet=True)

# Function for text standardization
def standardize_text(input_text):
    cleaned_text = []

    for text in input_text:
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = text.replace('\n',' ') # Remove New Lines
        text = text.strip() # Remove Leading White Space
        text = re.sub(' +', ' ', text) # Remove multiple white spaces
        text = re.sub(r'(?<=[a-zA-Z])(\d+)', ' ', text)  # Mengganti angka yang menempel di kata dengan spasi
        text = re.sub(r'[^\w\s,]', '', text)

        # Tokenize the ingredient
        tokens = word_tokenize(text.lower())

        # Unit normalization mapping
        replacements = {
            "mnjd": "menjadi",
            "mnjdi": "menjadi",
            "7buah": "7 buah",
            "baw": "bawang",
            "uk": "ukuran",
            "btng": "batang",
            "pre": "per",
            "bungkus": "bungkus",
            "sy": "saya",
            "krn": "karena",
            "g": "tidak",
            "sya": "saya",
            "kecup": "kecap",
            "q": "aku",
            "pke": "pakai",
            "baput": "bawang putih",
            "bamer": "bawang merah",
            "jgn": "jangan",
            "blackpaper": "blackpepper",
            "mecin": "micin",
            "mayones": "mayonaise",
            "lalau": "lalu",
            "jg": "juga",
            "gak": "tidak",
            "utk": "untuk",
            "sedikiiiiiit": "sedikit",
            "brp j": "beberapa saja",
            "lecil": "kecil",
            "cage": "cabe",
            "sacet": "sachet",
            "purih": "putih",
            "b putih": "bawang putih",
            "siuang": "siung",
            "secukup ny": "secukupnya",
            "sckpnya": "secukupnya",
            "secukupnyaa": "secukupnya",
            "bwg": "bawang",
            "yaa": "",
            "gendot": "gendut",
            "bks": "bungkus",
            "memumis": "menumis",
            "gls": "gelas",
            "n": "dan",
            "kalo": "kalau",
            "tmbh": "tambah",
            "sdkit": "sedikit",
            "ajjah": "aja",
            "mw": "mau",
            "ga": "tidak",
            "pake": "pakai",
            "gg": "tidak",
            "papa": "apa",
            "pakek": "pakai",
            "b merah": "bawang merah",
            "yiest": "yeast",
            "sere": "serai",
            "durix": "durinya",
            "dg": "dengan",
            "geprak": "geprek",
            "dihlskan": "dihaluskan",
            "60grm": "60 gram",
            "jd": "jadi",
            "btang": "batang",
            "tengiri": "tenggiri",
            "hlus": "halus",
            "pk": "pakai",
            "buncir": "buncis",
            "bw": "bawang",
            "poton2": "potong-potong",
            "cui": "cuci",
            "bngks": "bungkus",
            "lgsg": "langsung",
            "yg": "yang",
            "gw": "saya",
            "sdh": "sudah",
            "dun": "dan",
            "sendok mkn": "sdm",
            "seuai": "sesuai",
            "sct": "sachet",
            "cengkih": "cengkeh",
            "45menit": "45 menit",
            "daunbawang": "daun bawang",
            "sledri": "seledri",
            "mrica": "merica",
            "klo": "kalau",
            "gk": "tidak",
            "paz": "pas",
            "sckup y": "secukupnya",
            "dremas²": "diremas",
            "ketiting": "keriting",
            "laf nte": "lafonte",
            "ati": "hati",
            "sckupnya": "secukupnya",
            "kurleb": "kurang lebih",
            "krna": "karena",
            "beking": "baking",
            "sbg": "sebagai",
            "3bh": "3 buah",
            "sedikiiiit": "sedikit",
            "lb": "lembar",
            "sckpny": "secukupnya",
            "tp": "tapi",
            "me": "merk",
            "sdikit": "sedikit",
            "sahcet": "sachet",
            "sj": "saja",
            "msh": "masih",
            "dlarutkan": "dilarutkan",
            "ak": "aku",
            "secukupnyq": "secukupnya",
            "2bh": "2 buah",
            "1bh": "1 buah",
            "sblm": "sebelum",
            "1buah": "1 buah",
            "4cup": "4 cup",
            "unt": "untuk",
            "2sdm": "2 sdm",
            "stengah": "setengah",
            "btr": "butir",
            "sckpnya|sekucupnya|scukupnya|sckupnya|se cukup nya|secukup nya|secukupny|sckupx": "secukupnya",
            "sckp": "secukup",
            "bks|bngkus|bgks|bgkus": "bungkus",
            "utk|tuk": "untuk",
            "sdm|adm": "sendok makan",
            "sbgai|sbg": "sebagai",
            "bh|bua": "buah",
            "gls": "gelas",
            "lbr|lebar|lmbar|lbar": "lembar",
            "bamer|bsngmer": "bawang merah",
            "baput|b.putih": "bawang putih",
            "sdt": "sendok teh",
            "cm": "centimeter",
            "ml": "mililiter",
            "kg": "kilogram",
            "btg|bt": "batang",
            "bwg|bw": "bawang",
            "pth": "putih",
            "mrh": "merah",
            "dg": "dengan",
            "pcs": "pieces",
            "uk": "ukuran",
            "bj": "biji",
            "tdk|g": "tidak",
            "d": "di",
            "gr": "gram",
            "pke": "pakai",
            "cabe": "cabai",
            "rb": "ribu",
            "g(?= minyak)": "goreng",
            "potoh": "potong",
            "adonanya": "adonannya",
            "kcl": "kecil",
            "mkn": "makan",
            "bs": "bisa",
            "ditambh": "ditambah",
            "jk": "jika",
            "ptong": "potong",
            "diptong": "dipotong",
            "yng": "yang",
            "yh": "ya",
            "kaka": "kakak",
            "bnyak": "banyak",
            "mantaaapppp": "mantap",
            "sma": "sama",
            "mentaga": "mentega",
            "bg": "bagian",
            "dl": "dulu",
            "seh": "sih",
            "ikanasin": "ikan asin",
            "aq|ak|q": "aku",
            "kl|klo": "kalau",
            "yaa": "ya",
            "kcg": "kacang",
            "pnjag": "panjang",
            "skanya": "sukanya",
            "seray": "serai",
            "saledri": "seledri",
            "aj": "aja",
            "lebkur": "kurang lebih",
            "lg": "lagi",
            "dr": "dari",
            "padas": "pedas",
            "bwt": "buat",
            "dn": "dan",
            "sd": "sudah",
            "jeru": "jeruk",
            "blimbing": "belimbing",
            "sc|sct|saset": "sachet",
            "g(?= \\d+)": "gram",
            "klau": "kalau",
            "jgn": "jangan",
            "butit": "butir",
            "mrica": "merica",
            "toge|tauge": "taoge",
            "bombay": "bombai",
            "ptg": "potong",
            "kriting": "keriting",
            "ijo": "hijau",
            "sndk|sndok": "sendok",
            "mie": "mi",
            "trigu": "terigu",
            "sambel": "sambal",
            "mayonais": "mayones",
            "crispi": "crispy",
            "bbuk": "bubuk",
            "pete": "petai",
            "margarine": "margarin",
            "lenguas": "lengkuas",
            "caos|saos": "saus",
            "blimbing": "belimbing",
            "mosarela": "mozzarella",
            "kp": "kepala",
            "otak-ota": "otak-otak",
            "metga": "mentega",
            "teriyki": "teriyaki",
            "ati": "hati",
            "sdk": "sendok",
            "uhkuran": "ukuran",
            "susku": "susu kental manis",
            "jintan": "jinten",
            "keprek": "geprek",
            "lon": "lonjong",
            "dipingg": "dipanggang",
            "kiub": "kubus",
            "elgy": "lagi",
            "tekelar": "teflon",
            "kpja": "kecap",
            "bubule": "berlumpur",
            "bodo": "bodoh",
            "knykn": "kenyamanan",
            "pgn": "pinggan",
            "pnggg": "pengetahuan",
            "ckli": "cek",
            "pados": "padahal",
            "bekel": "bekel",
            "pndah": "pernah",
            "krwg": "kerawang",
            "ny": "ini",
            "ktngy": "ketinggalan",
            "rembel": "gambar",
            "sckpg": "sakit",
            "blkny": "baliknya",
            "wngl": "bolong",
            "snangin": "suaminya",
            "nra": "cuman",
            "cepath": "cepat",
            "serca": "sampe"
        }

        # Standardize units and remove quantities
        standardized_tokens = []
        for token in tokens:
            # Remove quantities (check for numeric tokens)
            if not token.isnumeric():
                # Normalize units
                if token in replacements:
                    token = replacements[token]
                standardized_tokens.append(token)

        # Lemmatization
        lemmatizer = WordNetLemmatizer()
        standardized_tokens = [lemmatizer.lemmatize(token) for token in standardized_tokens]

        # Stop word removal
        stop_words = set(stopwords.words("english"))
        standardized_tokens = [token for token in standardized_tokens if token not in stop_words]

        # Words to remove
        common_words= ['kg', 'gram', 'bahan', 'potong', 'yang', 'secukupnya', 'buah', 'siung', 'sdm', 'sendok', 'daun', 'teh', 'butir', 'bumbu', 'iris', 'bubuk', 'sendok teh',
                       'halus', 'goreng', 'ruas', 'lembar', 'bh', 'manis', 'batang', 'untuk', 'selera', 'dan', 'centimeter', 'sesuai']
        standardized_tokens = [token for token in standardized_tokens if token not in common_words]

        # Join the standardized tokens back to form the ingredient string
        standardized_text = " ".join(standardized_tokens)

        cleaned_text.append(standardized_text)

    return cleaned_text

import nltk
nltk.download('punkt_tab')

standardized_ingredients = standardize_text(merged_data['ingredients'])

"""anjay bisa gusy

## Modeling
"""

import re
import pandas as pd
import numpy as np
from gensim.models import FastText
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import word_tokenize

# Function to remove patterns from text
def remove_patterns(list_text):
    text_wo_pattern_lst = []
    for text in list_text:
        tokens = text.split()
        tokens = [re.sub(r'\d+$', '', token) for token in tokens]
        tokens = [token for token in tokens if not re.match(r'^\d+[A-Za-z]+\d+$', token)]
        filtered_tokens = [token for token in tokens if not re.match(r'\d+(?:\/\d+)?(?:[a-z°°]+)?', token)]
        filtered_text = " ".join(filtered_tokens)
        text_wo_pattern_lst.append(filtered_text)
    return text_wo_pattern_lst

# Clean and preprocess text
all_text = merged_data['ingredients'] + ' ' + merged_data['Steps']
cleaned_ver = remove_patterns(all_text)
merged_data['cleaned_text'] = cleaned_ver
merged_data['clean_tokens'] = [word_tokenize(text) for text in cleaned_ver]

# Step 1: Filter data by category
def filter_by_category(data, selected_categories):
    return data[data['kategori'].isin(selected_categories)].reset_index(drop=True)

# Step 2: Filter data by allergens
def filter_by_allergens(data, user_allergens):
    filtered_data = data.copy()
    for allergen in user_allergens:
        filtered_data = filtered_data[~filtered_data['ingredients'].str.contains(allergen, case=False)]
    return filtered_data.reset_index(drop=True)

# Step 3: Recommend recipes based on user input ingredients using FastText
def recommend_recipes_fasttext(data, user_input, fasttext_model, top_n=5):
    # Preprocess user input
    user_input_tokens = word_tokenize(user_input)

    # Generate user input vector
    user_input_vector = np.zeros(fasttext_model.vector_size)
    num_tokens = 0
    for token in user_input_tokens:
        if token in fasttext_model.wv:
            user_input_vector += fasttext_model.wv[token]
            num_tokens += 1
    if num_tokens > 0:
        user_input_vector /= num_tokens

    # Generate recipe vectors
    recipe_vectors = []
    for recipe_tokens in data['clean_tokens']:
        recipe_vector = np.zeros(fasttext_model.vector_size)
        num_tokens = 0
        for token in recipe_tokens:
            if token in fasttext_model.wv:
                recipe_vector += fasttext_model.wv[token]
                num_tokens += 1
        if num_tokens > 0:
            recipe_vector /= num_tokens
        recipe_vectors.append(recipe_vector)

    # Calculate cosine similarities
    recipe_vectors = np.array(recipe_vectors)
    similarities = cosine_similarity([user_input_vector], recipe_vectors)[0]

    # Combine scores with Loves column
    max_loves = data['Loves'].max()
    data['normalized_loves'] = data['Loves'] / max_loves
    combined_scores = 0.8 * similarities + 0.2 * data['normalized_loves']

    # Create recommendation list
    data['score'] = combined_scores
    recommendations = data.nlargest(top_n, 'score')[['Title', 'Loves', 'score', 'ingredients']]
    return recommendations

from googletrans import Translator

translator = Translator()

def detect_and_translate(user_input):
    detected_language = translator.detect(user_input).lang
    if detected_language == 'id':
        # Input is already in Indonesian
        return user_input
    else:
        # Translate from English to Indonesian
        words = [word.strip() for word in user_input.split(",")]
        translated_words = [translator.translate(word, src='en', dest='id').text for word in words]
        return ", ".join(translated_words)

def main2(merged_data, selected_categories=None, user_allergens=None, user_ingredients=None):
    # Train FastText model
    all_tokens = merged_data['clean_tokens']
    fasttext_model = FastText(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)

    # Step 1: User selects categories
    if not selected_categories:
        raise ValueError("No categories selected!")
    filtered_data = filter_by_category(merged_data, selected_categories)

    # Step 2: User inputs allergens
    if user_allergens:
        user_allergens = detect_and_translate(user_allergens)  # Translate allergens if needed
        user_allergens = [allergen.strip() for allergen in user_allergens.split(',')]
        filtered_data = filter_by_allergens(filtered_data, user_allergens)

    # Step 3: User inputs ingredients
    if not user_ingredients:
        raise ValueError("No ingredients entered!")
    user_ingredients = detect_and_translate(user_ingredients)  # Translate ingredients if needed
    recommendations = recommend_recipes_fasttext(filtered_data, user_ingredients, fasttext_model)

    # Return recommendations
    return recommendations
